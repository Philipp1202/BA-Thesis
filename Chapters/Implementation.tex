% !TEX root = ../Thesis.tex
\chapter{Implementation}

In this section, we will introduce how we implement a word-gesture keyboard using Unity, a Python script and a simplified version of the word detecting algorithm used in $\text{SHARK}^2$.

\section{Word Graph Generator}
As previously discussed in \Cref{SHARK2}, to make the word detecting algorithm used in $\text{SHARK}^2$ work, the perfect graphs for all words in a lexicon must be precomputed. Therefore, we need a script, that either creates or overwrites a file for every available keyboard layout. It has to write the words included in the lexicon together with the corresponding $N$ sampled points of their graphs. Per line, such a file contains a word, then a certain number $N$ of points from the word's perfect graph followed by the same points, but normalized. Normalized here means the same as mentioned in the background section \ref{normalize}.\\
To run the script, the user has to provide the name of the layout that they want to create the perfect graphs for. Additionally, they also have to write the name of the text file containing all the words, which is the lexicon. The script then either creates a new file named ``graph\_\textit{layout}.txt'' or if already a file with this name exists, it deletes its content. Then it fills the file line by line as mentioned above. In further sections, we will call a file of this type ``layout file''. The script can only be executed for one layout at a time. Hence, if there are multiple available layouts for our word-gesture keyboard, the user has to run the script for every single one. One thing the script pays attention to is, if a word can be written with used layout. If there are words in the lexicon, that can not be written with the given layout, the script skips them. Hence, there will be no line in the file for said word. 

\section{Used Algorithm}
For our word-gesture keyboard we use a simplified version of the algorithm used in $\text{SHARK}^2$. This means, we also work with two core channels, a location recognizer and a shape recognizer, but do not implement everything from the algorithm used in $\text{SHARK}^2$. The shape recognizer is to calculate the deviation from the user inputted graph and a perfect graph from a word with respect to their shape. The location recognizer is for the same thing, but not with respect to the shape, but rather the position on the keyboard. When looking at the shape, we normalize the graphs in a specific way as explained in \Cref{normalize}, so the position, where they exactly lie on the keyboard, does not matter. When looking at the location, we look at the graphs as they are, without normalizing or changing anything.\\
As in the $\text{SHARK}^2$ system we also use the start and end positions of the graphs as a first pruning method. The difference is, that for $\text{SHARK}^2$, the authors chose to normalize all the graphs in scale and translation before computing the start-to-start and end-to-end distances. In our algorithm, we do not normalize the graphs, but just look at the start and end positions of a user inputted graph and a word's perfect graph. As threshold we set the width of a key. This means, if either the start-to-start or end-to-end distance is bigger than a key width, the word to which this applies, gets discarded.\\
Another thing we implement differently is $\sigma$. For the channel integration formula \ref{eqn:gaussian}, they use $\sigma$ in $\text{SHARK}^2$ as a parameter. They determine its value by the gesturing speed mentioned in \Cref{gesturing speed}. We do not use the gesturing speed in our algorithm. For the location channel (in the formula \ref{eqn:gaussian}) we use the radius of a key on the keyboard as value for $\sigma$. For the shape channel (in the formula \ref{eqn:gaussian}) we use a value that equals to the radius of a normalized key. That means, a small graph will have a bigger $\sigma$ than a big graph, because we normalize the graph's longer bounding box side to a fixed length. Therefore, a small graph gets stretched, whereby a big graph gets contracted.\\
As mentioned in the beginning of this section, we use a simplified form of the algorithm used in $\text{SHARK}^2$. That being said, we do currently not use any language information nor dynamic channel weighting by gesturing speed. We do not use any language information, because that would have gone beyond the scope of this project. The dynamic channel integration was not implemented by us, because in our opinion the value of sigma, as we chose it, is fine for the purpose of our word-gesture keyboard in VR.

\section{Functions}
In this section, we will present the functions our word-gesture keyboard provides.
\begin{figure}
\centering
\includegraphics[width=0.7\textwidth]{WGKKeyboard.png}
\caption{Word-gesture keyboard in its ``normal'' form without anything selected}
%\label{fig:machine}
\end{figure}
    
\subsection{Text input}
There should be two text input methods for the user. One is the input of words with gestures, the other one is the input of single characters by tapping single keys.

\begin{figure}[H]
    \makebox[\textwidth][c]{
        \centering
        \subbottom[The user is writing the word ``hello'' with a gesture\label{fig:write_suggestions:write_suggestions1}]{\includegraphics[width=0.4\textwidth]{WGKWordWrite.png}}\hspace{1.0em}
        \subbottom[The user gets recommended words for ``and''\label{fig:write_suggestions:write_suggestions2}]{\includegraphics[width=0.4\textwidth]{WGKWordSuggestions.png}}
    }
    \caption{In (a) a user is writing the word ``hello'' with a gesture, but did not release the trigger yet and can still see the red line, which indicates the path they draw. In (b) a user already wrote the word ``and'' with a gesture and can now see three word suggestions they could choose from.}
    \label{fig:write_suggestions}
\end{figure}

\subsubsection{With Gestures}
The most important function our word-gesture keyboard provides, is the writing of words with gestures. A user can press and hold the trigger button of a VR controller inside the keyboard's hitbox and start making a gesture on it. The user will see a red line, as seen in \cref{fig:write_suggestions:write_suggestions1}, that is drawn on the keyboard where they gesture. This helps them to keep track of the line they drew. When the user wants to finish the gesture, they need to release the trigger of the controller. At this moment, our program starts to evaluate up to 5 words with the best match to the user inputted graph. The one with the highest confidence score will be written into a chosen text field. Up to 4 other words are displayed at the keyboard \cref{fig:write_suggestions:write_suggestions2}, such that the user can also choose between them by touching the right button with their controller. When they choose one of these word suggestions, the word that has been written into the text field before, is replaced by the chosen word and the button, where the chosen word was written, will then display the replaced word.\\
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{BA3DTo2D.pdf}
    \caption{Transformation of a graph/gesture from the 3D space into the 2D space}
    \label{fig:3DTo2D}
    \end{figure}
There are some challenges in VR we have to master. First of all, in VR the user is in a three-dimensional space. Thus, when they make a gesture, they could move up and down with their controller and do not have to stay on the keyboard. For this we need to look, that the line of a gesture still lies on the keyboard as if the user would draw on a piece of paper by using a pen. The next problem that follows is, that a user can rotate the keyboard as they want in VR. Therefore, the drawn graph will most of the time not be lieing perfectly on the x-z-plane even though it is drawn flat on the keyboard's surface. This means, we have to transform all the points into the x-z-plane to be able to further process these gestures. Another problem of drawing gestures is the amount of points. Technically seen, every frame the user draws a gesture, a new point gets saved. Thus, we have to reduce this amount, because it would be just too many points, and it would slow down the program massively. We implement a function that always keeps new points but either replaces the second-newest one or gets added to the point list. This is determined by the distance between the second-newest point and the third-newest one. If the distance between them is smaller than a minimum segment distance, there is a replacement otherwise there is an addition of the newest point.TODO

\subsubsection{As Single Characters}
If the wanted word is not in the lexicon, there will not be any entry in any layout file. Therefore, our algorithm will not be able to get this wanted word as best match, hence it can not be written as gesture. For this case, we need a method to input single characters. Fortunately with our word-gesture keyboard this almost works without additional work. If the single letters are in the layout files, the user is technically seen able to write single letters with a gesture. This ``gesture'' would just be a point on the right key, hence a single click at the right position. But there might be a little inconvenience. This is caused by the fact, that we work with distances. The distance from one letter to another is not big. And if for example the user wants to write an ``e'' but presses the key with the ``e'' on it on its left side and not perfectly in the middle, our system would evaluate that aside from ``e'', also ``w'' and ``we'' are words, the user might have intended to write (on a conventional QWERTZ or QWERTY layout). To avoid this, our system checks, if the user inputted graph's bounding box is smaller than the radius of a key on the keyboard. If so, it recognizes that the user wants to write a single letter, and then takes the best match. To get back to the example, ``we'' would be discarded and ``e'' would get a higher score than ``w'', because of a smaller location channel distance. Therefore, the written ``word'' in this case would be ``e''.

\subsubsection{Spaces and Backspaces}
Important when writing more than one word are spaces. The proper way to automatically set them would be, if a word of more than one character was written with a gesture, for the next input, not matter if it is a single character or a whole word, a space is put before it. Some exceptions where the space would be omitted is for example if after a word a ``.'', ``,'' or ``?'' is written. Also, at the start of a sentence, it should not set a space. If a single character was written, and the next input is a word consisting of more than one character it should set a space before it, but if again a single character follows, it should not put one.\\
In our implementation, when the user writes a word that consists of more than one character, a space gets put behind the word. This means, if they write a second word or a single character after, they do not have to put a space manually. But if they write a single character, no space is put automatically after it. We do it this way, because the keyboard does not know what was already inputted in a textfield. For example, if a user writes something into a textfield and then switches to another one, there could be problems because the keyboard itself does not know that the textfield has been changed and then might write a space where there should be none.\\
When the user writes a word and then uses the backspace key, the whole word including the space gets deleted. After this first word, only single characters will be deleted afterwards.

\subsection{Create New Layouts}
Another function is the creation of custom layouts. While this would not be a necessary function to reach the core goal of inputting text, it can still be helpful for users to feel more confident using the keyboard. For example, if we only implement the QWERTZ layout, a user that is only using keyboards with the QWERTY, ATOMIK or any other layout than QWERTZ, might have problems using it. The problem is, that we do not know every layout all users are using for non-VR text input. Therefore, it is good that a user can decide by themselves what layout they prefer to use and simply create it for our word-gesture keyboard.\\
To create new layouts, a text file that contains all available layouts exists. The user can create as many new layouts as they want to. To create a new one, the user has to write the new layout's name on the first new line. On the following lines they have to write the characters in an order, in which they want to have them on the keyboard. Then there have to follow two ``\$'' and a number. This number will put a space from the left side. So if it is for example a 2, the line will be indented by two key widths. All the Unicode characters are working, but two. In the current implementation, one whitespace is used to declare the position of the spacebar and the ``$<$'' character is used for the backspace key. This file gets read at the start of the program, so it cannot be edited while the program is running, or to be precise, the changes will not be recognized during runtime. 
%One smaller thing we implemented is, that at the start of the program all characters used in the layouts not yet in the lexicon text file and layout files are being added. Without doing this, the user might not be able to input some single characters with their newly created keyboard, because the system simply would not find them in the layout files. During the runtime, the user then is able to switch between available layouts.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{WGKKeyboardLayout.png}
    \caption{Experimental alphabet keyboard}
    \label{fig:WGKLayout}
\end{figure}

\subsection{Options}

\begin{figure}[H]
    \makebox[\textwidth][c]{
        \centering
        \subbottom[The pressed options button with the three main options\label{fig:options:options1}]{\includegraphics[width=0.25\textwidth]{WGKOptions4.png}}\hspace{3.0em}
        \subbottom[The pressed options button with all sub-buttons\label{fig:options:options2}]{\includegraphics[width=0.247\textwidth]{WGKOptions5.png}}
    }
    \caption{The options button with the main buttons not activated in (a) and activated in (b)}
    \label{fig:options}
\end{figure}

To let the user use the other functions, we implement an options button for our keyboard, which is marked with a black gear as seen in \cref{fig:options:options1}. The user can click on it with the trigger button of the controller when being in its hitbox. Then, three new buttons appear, each for one currently available option. One is to add new words, one to change the currently selected layout and the last one to scale the keyboard.

\subsubsection{Add Words}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{WGKAddWord2.png}
    \caption{``vitrivr'' written with single character inputs in the ``add word mode''}
    \label{fig:addword}
    %\label{fig:machine}
\end{figure}
The whole system works with a lexicon full of words and only these can be written with gestures. There will be words the user wants to write, that are not yet in said lexicon, hence they can not be written with gestures. For this case, we implement a function such that the user can add new words. They can access it via the options button. Then they have to press the button where it says ``add word''. When this button is pressed another button appears on the right side of the keyboard, the ``add to lexicon'' button. Now, the user has to input their intended word with single characters. It will be displayed as seen in Figure \ref{fig:addword}. Then, when they press the ``add to lexicon'' button, the displayed word will be added to the lexicon text file, if it does not already exist, and does not have a space in it. Additionally, for every available layout, the newly added word is also added to the corresponding layout file, so that the user can write the word with a gesture right after adding it. One additional thing we implement is, that the word only gets added to the layout files, if it can be written with the layout the file corresponds to. For example, if a user wants to add the word ``öffentlich'', but they made a layout without the letter ``ö'', this word could never be written with this layout, hence it would be unnecessary to have it in the corresponding layout file.

\subsubsection{Change Scale and Layout}
There are two other functions that can be found under the options button. One is to change the size of the keyboard. When the user clicks on the ``change scale'' button, a ``+'' and a ``-'' button appear. By pressing the ``+'' button, the keyboard gets bigger, by pressing the ``-'' button, the keyboard gets smaller. This can help the user to make the keyboard more handy for them. Depending on how they want to move their arm, they can make it slightly bigger or smaller. If it is bigger, the gestures can be made more precise, if it is smaller, gestures can be made faster. The ``+'' and ``-'' buttons can be seen in \cref{fig:options:options2}\\
The other function is the ability to change the layout. The user gets a list of all available ones, when pressing the ``change layout'' button. They can then click on one of these buttons and the keyboard will change its appearance. The available layouts consist of the predefined ones and also of the newly added ones from the user. In \cref{fig:options:options2} there is a long list of available layouts visible.

\subsection{Moving, Rotating and Word Preview}
Finally, we will briefly talk about the last two functions. One is the ability to grab and move the keyboard. This means, the user can grab the keyboard, by being in its hitbox and pressing and holding the controller's grip button. They can move it around in the room and rotate it as they want. If they release the grip button, the keyboard gets static and stays in the position.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{WGKKeyboardMove.png}
    \caption{The user has the keyboard in the hand and moves and rotates it around.}
    \label{fig:move}
\end{figure}
This function can become handy, if the user wants to reposition themselves in the VR space. They can just take the keyboard with them. Also, they can choose to have it closer or further away, which can make it more convenient to use the keyboard. Rotating the keyboard can also help this purpose. Some users might prefer to have it more tilted than others.\\
The other function is that the keyboard shows a preview word in real time while a user makes a gesture. This means, it shows the word the algorithm would detect as best match, if the user released the trigger. This can be seen at the top of \cref{fig:write_suggestions:write_suggestions1}, where ``hello'' is written as ``word preview''.